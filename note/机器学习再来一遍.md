### [终于可以可视化理解feature map了(从第10分钟开始看)](https://www.bilibili.com/video/av17421632/?p=16)

### [然后再看这个](https://www.bilibili.com/video/av20381986/?p=81)

**关于逻辑回归，SVM 和 神经网络的关系，看李宏毅的 Lecture 5 和 Lecture 20,讲的超级赞**

![image-20181022180606066](/Users/yunqingqi/Desktop/note/image-20181022180606066.png)

![image-20181022182229895](/Users/yunqingqi/Desktop/note/image-20181022182229895.png)

如何解决bias过高的问题：加入更多feature   或者  让模型更复杂一点

![image-20181022183113426](/Users/yunqingqi/Desktop/note/image-20181022183113426.png)

这就是decay参数的来源，是为了自适应调整learning rate，而不是不变

L2 的理解：

![image-20181022184348065](/Users/yunqingqi/Desktop/note/image-20181022184348065.png)

![image-20181022185535631](/Users/yunqingqi/Desktop/note/image-20181022185535631.png)

![image-20181022185848544](/Users/yunqingqi/Desktop/note/image-20181022185848544.png)

**上图 体现了 property 2**

![image-20181022190341952](/Users/yunqingqi/Desktop/note/image-20181022190341952.png)

**上面的例子就是一个卷积核每次只和9个输入（左上角的一片区域，也可以理解为，只在一小片区域里找特征pattern，这里可以理解为体现了property 1）卷积，所以不是FCN**

![image-20181022192429588](/Users/yunqingqi/Desktop/note/image-20181022192429588.png)

![image-20181022195504214](/Users/yunqingqi/Desktop/note/image-20181022195504214.png)

为什么越深越好？ 用很简单的例子来解释一下：比如我们现在想要分类：长发女，长发男，短发女，短发男，如果我们直接把数据放进去训练，因为长发男的数据比较少，这样的分类器肯定很弱。继续往下看

![image-20181023174158569](/Users/yunqingqi/Desktop/note/image-20181023174158569.png)

在这里，我们可以让网络去分类：是男or女，是长发／短发？这样的话，每个分类器的训练数据不会太不平衡

![image-20181023174502248](/Users/yunqingqi/Desktop/note/image-20181023174502248.png)

这样的话，就相当于我们是2层分类器了，第一层是最基本的（男or女，是长发／短发？）

![image-20181023174350582](/Users/yunqingqi/Desktop/note/image-20181023174350582.png)

这也就解释了为什么越深越好，因为每一层的分类都建立在前一层的分类基础上

![image-20181023174116576](/Users/yunqingqi/Desktop/note/image-20181023174116576.png)



这个就是       逻辑回归：

![image-20181024162458598](/Users/yunqingqi/Desktop/note/image-20181024162458598.png)

![image-20181024165840250](/Users/yunqingqi/Desktop/note/image-20181024165840250.png)![image-20181024165910399](/Users/yunqingqi/Desktop/note/image-20181024165910399.png)

![image-20181024170356980](/Users/yunqingqi/Desktop/note/image-20181024170356980.png)

**Cross-entropy 是说 这两个distribution有多像，如果很像，那么就为值就为0**

![image-20181024170733384](/Users/yunqingqi/Desktop/note/image-20181024170733384.png)

![image-20181024171435968](/Users/yunqingqi/Desktop/note/image-20181024171435968.png)

![image-20181024171656171](/Users/yunqingqi/Desktop/note/image-20181024171656171.png)

**我们可以看到，logistic regression 和 linear regression 分别用 corss-entropy 和 square loss 时 他们的w 更新公式都是一样的。 下面我们来回答，为什么logistic regression要用corss-entropy 而不是 square loss，从下图我们可以看到 当预测值 和 真是值十分相近或者一样或者时分远的时候，w的更新都是0，所以不可以用square loss！！！**

![image-20181024172014237](/Users/yunqingqi/Desktop/note/image-20181024172014237.png)

![image-20181024172514837](/Users/yunqingqi/Desktop/note/image-20181024172514837.png)

**从这个图也可以看出来，当我们的预测和真实值非常大的时候，cross-entropy的gradient很大，而square error的gradient极小。。不利于更新～～～～**

![image-20181024173109383](/Users/yunqingqi/Desktop/note/image-20181024173109383.png)

![image-20181024173527982](/Users/yunqingqi/Desktop/note/image-20181024173527982.png)

![image-20181024174240016](/Users/yunqingqi/Desktop/note/image-20181024174240016.png)

**但是 逻辑回归 有一些限制，比如上面4个点，我们去用 逻辑回归 分类，它不可能把点分对，只能分成如下这样：（原因老师没讲，我觉得其实是特征太少，这里就2个特征，所以我们的w也就只有w1 和 w2 两个，无法模拟复杂一点的模型），即使是这样，我们也可以想办法解决，那就是 feature transformation！！！**

![image-20181024174335521](/Users/yunqingqi/Desktop/note/image-20181024174335521.png)



**下面看看怎么transform的**

![image-20181024174548260](/Users/yunqingqi/Desktop/note/image-20181024174548260.png)

**所以解决办法就是 将 多个 逻辑回归 连起来 （第一个负责训练feature transform，第二个负责分类）**：

![image-20181024174838398](/Users/yunqingqi/Desktop/note/image-20181024174838398.png)

**是不是感觉超级像神经网络～～！！！！！！！，其实神经网络就是这么发展来的！！！**

![image-20181024175244519](/Users/yunqingqi/Desktop/note/image-20181024175244519.png)

**下面来看这一张图：**

![image-20181024180330787](/Users/yunqingqi/Desktop/note/image-20181024180330787.png)

**横轴代表的是![image-20181024180204101](/Users/yunqingqi/Desktop/note/image-20181024180204101.png) ， 纵轴代表的是loss，因为我们总是希望我们的预测值 $f(x)$ （结果得为正则output为1，结果为负则output为-1） 和  $\hat{y}^n$ （结果是 -1 或 1）同号，同号的话也就意味着他们的乘积在x正半轴，我们希望越大越好（也就是说f(x)预测的很好），此时loss很低！！！！！！，如果不同号，则 $\hat{y}^nf(x)$   会是负数，也就是说他们俩乘积在x负半轴，此时loss很大！！！** 这就是我们所期望的 loss function～～，也就是图中的黑色线，但是这条线不好求导。

**但是如果我们用square loss的话，又不合理：因为$\hat{y}^nf(x)$ 在正半轴的时候，它的loss反而很大**

![image-20181024181500353](/Users/yunqingqi/Desktop/note/image-20181024181500353.png)

**但是如果我们取 sigmod + square 作为 loss的时候，又会出现相同的问题，就是说在很接近的真实值和 离真实值非常远的时候，gradient很小，不利于更新，那么我们可以考虑 sigmod + cross-entropy：**

![image-20181024182549510](/Users/yunqingqi/Desktop/note/image-20181024182549510.png)

**来看一下 SVM 的hinge loss**

![image-20181024183733815](/Users/yunqingqi/Desktop/note/image-20181024183733815.png)

**比较看一下 cross-entropy 和 hinge loss，看黑点，cross entropy在黑点的地方还有损失，也就是说我不紧要好，我还要更好，而hinge loss 就是说我要大致好就行，所以svm会不太在意outlier**

![image-20181024184228462](/Users/yunqingqi/Desktop/note/image-20181024184228462.png)

![image-20181024190528878](/Users/yunqingqi/Desktop/note/image-20181024190528878.png)

**逻辑回归 和 linear SVM 的区别就是用了不同的loss function**

![image-20181024191717265](/Users/yunqingqi/Desktop/note/image-20181024191717265.png)

**现在结合 hinge loss 再来 看一下 这个式子![image-20181024191749292](/Users/yunqingqi/Desktop/note/image-20181024191749292.png)，我们现在就知道了，为什么这样可以使 margin 更大，为什么后面那个 item 我们叫它松弛因子了，因为 当 $\hat{y}^nf(x)$ = ![image-20181024191945440](/Users/yunqingqi/Desktop/note/image-20181024191945440.png)的时候，我们就没有损失了，正常情况下 $\hat{y}^nf(x)$ 是要 >=1  才可以的（结合图就很明白了）**

其实 linear SVM 是可以用 gradient descnet 来求解的

![image-20181024194101217](/Users/yunqingqi/Desktop/note/image-20181024194101217.png)

![image-20181024194256925](/Users/yunqingqi/Desktop/note/image-20181024194256925.png)

**在这个图的上面就有 hinge loss 的导数公式，（从gradinet decent角度来看）我们可以看到，其实已经分对的点，$c^n(w)$ 的值就是0，也就是说分对的点对 w 的更新根本没影响，也就是说他们没有对分类有贡献，也就是 svm 对分对的点就不考虑了，而逻辑回归 是每个点都会去看，所以 SVM 更加 robust ！！！！**

![image-20181025134918471](/Users/yunqingqi/Desktop/note/image-20181025134918471.png)

![image-20181025134942350](/Users/yunqingqi/Desktop/note/image-20181025134942350.png)

![image-20181025142047681](/Users/yunqingqi/Desktop/note/image-20181025142047681.png)

**所以说，我们只需要知道 $K(x^n,x)$ 的值（因为此时认为 $a_n$ 已经算出来了），我们就可以对该点进行预测**

![image-20181025152314539](/Users/yunqingqi/Desktop/note/image-20181025152314539.png)

**我们可以把低维的 x 投射到高维，然后再计算，但是我们发现，有时候直接计算kernel，比先投影再计算要快 ，比如下面这个，kernel的功能就是把 x和z相乘再平方**

 ![image-20181025153137988](/Users/yunqingqi/Desktop/note/image-20181025153137988.png)

**比如下面这个kernel 功能就是可以把 x 和 z 投射到无穷多维，但是无穷多维我们根本想不到（泰勒展开式），所以根据上面得到的结论，我们直接求kernel的函数就好（这个kernel可以根据泰勒展开式论证，它可以把 x 和 z 投射到无穷多维）**

![image-20181025153424883](/Users/yunqingqi/Desktop/note/image-20181025153424883.png)

**如果我们把kernel 换成tanh，那么它就可以理解为是神经网络！！！！！！**

![image-20181025153752351](/Users/yunqingqi/Desktop/note/image-20181025153752351.png)

 PCA 例子：

![image-20181029143341795](/Users/yunqingqi/Desktop/note/image-20181029143341795.png)

![image-20181029143357954](/Users/yunqingqi/Desktop/note/image-20181029143357954.png)

![image-20181029143417134](/Users/yunqingqi/Desktop/note/image-20181029143417134.png)

**PCA是一种无监督的数据降维方法，与之不同的是LDA是一种有监督的数据降维方法**

**这是笔记上关于LDA 少的一点东西**

![image-20181029145958644](/Users/yunqingqi/Desktop/note/image-20181029145958644.png)

![image-20181029150017248](/Users/yunqingqi/Desktop/note/image-20181029150017248.png)

![image-20181029161907754](/Users/yunqingqi/Desktop/note/image-20181029161907754.png)



#### Kernel Principal Components Analysis 

![image-20181102183909938](/Users/yunqingqi/Desktop/note/image-20181102183909938.png)

![image-20181102183923775](/Users/yunqingqi/Desktop/note/image-20181102183923775.png)

![image-20181102183942965](/Users/yunqingqi/Desktop/note/image-20181102183942965.png)

![image-20181102183954407](/Users/yunqingqi/Desktop/note/image-20181102183954407.png)

投影的坐标是内积

![image-20181102184006702](/Users/yunqingqi/Desktop/note/image-20181102184006702.png)

但当我们计算的时候，一般就是这样求的：
$$
\frac{\vec{a} \cdot \vec{b}}{\left | b \right |} = \frac{\vec{a}^T \cdot \vec{b}}{\left | b \right |} = \frac{\vec{a} \cdot \vec{b}^T}{\left | b \right |}
$$
![image-20181102184017361](/Users/yunqingqi/Desktop/note/image-20181102184017361.png)

不同的数据集，他们的形状（分布）不同，需要的kernel也不同，如果训练集本来就分布很好，就没必要用kernel了

![image-20181102184031408](/Users/yunqingqi/Desktop/note/image-20181102184031408.png)

可以看到，我们特征向量可以是training sample 的线性组合

![image-20181102234409554](/Users/yunqingqi/Desktop/note/image-20181102234409554.png)

我们可以看到，最后求最大值的关键就是 $\lambda $ 的值，也就是说 $\lambda $ 的值越大越好，这就是为什么我们要取最大的前 $k$ 个特征值 对应的特征向量～～

![image-20181103010952611](/Users/yunqingqi/Desktop/note/image-20181103010952611.png)

![image-20181103011007795](/Users/yunqingqi/Desktop/note/image-20181103011007795.png)

![image-20181103102027157](/Users/yunqingqi/Desktop/note/image-20181103102027157.png)

![image-20181103101916683](/Users/yunqingqi/Desktop/note/image-20181103101916683.png)

消去 $\sum$ 的小技巧要会。。。。 ～～～，下面的也是：

![image-20181103102723117](/Users/yunqingqi/Desktop/note/image-20181103102723117.png)

![image-20181103104112048](/Users/yunqingqi/Desktop/note/image-20181103104112048.png)

在KPCA 的时候我们知道

![image-20181103104151281](/Users/yunqingqi/Desktop/note/image-20181103104151281.png)

所以我们的

![image-20181103104223227](/Users/yunqingqi/Desktop/note/image-20181103104223227.png)

![image-20181103110158208](/Users/yunqingqi/Desktop/note/image-20181103110158208.png)

